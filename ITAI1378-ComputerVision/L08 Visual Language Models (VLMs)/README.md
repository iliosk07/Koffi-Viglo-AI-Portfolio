# Exploring Visual Language Models (VLMs) Lab
This lab is about learning how Visual Language Models (VLMs) connect images and text. I worked in Google Colab, choosing either Path A with CLIP for zero-shot classification and image search, or Path B with BLIP/BLIP-2 for image captioning and visual question answering. I ran experiments to see how the models understand images, how embeddings work, and how different prompts or categories affect results. I also explored fine-tuning concepts, evaluated outputs using metrics like Recall@K or BLEU, and reflected on ethical considerations like bias, hallucination, and environmental impact.
From this lab, I learned that VLMs bridge vision and language through embeddings, enabling tasks like classification, search, and captioning without training on each specific dataset. I gained hands-on experience with model outputs, understood the importance of evaluation metrics, and learned the trade-offs between simpler models like CLIP and more resource-heavy models like BLIP-2. I also reflected on the ethical and societal implications of deploying AI that sees and speaks, realizing that even powerful models have limitations and biases. This lab gave me a practical understanding of how VLMs work and how they can be applied responsibly in real-world scenarios.
